{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled Dataset exploration, meta-data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{ASSUMPTIONS!!!.}}$ \n",
    "<br>\n",
    "$\\color{red}{\\text{We assume the zipped folder containing all the csv files is placed in thesame location as this notebook. }}$ \n",
    "<br>\n",
    "$\\color{red}{\\text{The current user has permission to copy and move files. Also can create folders}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "\n",
    "# Getting the necessary files ready\n",
    "import os\n",
    "\n",
    "# Provide the path to the data files\n",
    "data_path = \"/home/epb199/data/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some key functions\n",
    "\n",
    "#generator function\n",
    "def file_content(main_folder):\n",
    "    file_list = sorted(os.listdir(main_folder))\n",
    "    for file_name in file_list:\n",
    "        yield file_name\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fucntion to record file size and number of records\n",
    "\n",
    "#importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def record_stat(filepath):\n",
    "    \"\"\"Function receives a file path of a folder containing files as its argument. For each file,\\\n",
    "    the number of records and files size is calculated and two lists are return containing all these values.\\\n",
    "    This function also checks the schema of these files and group them by their schema\"\"\"\n",
    "    \n",
    "    file_size = [] #list to hold file size \n",
    "    file_rec = [] #list to hold file records\n",
    "    \n",
    "    for file in file_content(filepath):\n",
    "        file_path = os.path.join(filepath, file)\n",
    "        df = pd.read.csv(file_path, header=True)\n",
    "        \n",
    "        #Checking file size and number of records\n",
    "        file_rec.append(df.shape[0]) # append number of records\n",
    "       # x = os.stat(file_path).st_size\n",
    "        file_size.append(os.stat(file_path).st_size) # append each number of records\n",
    "    return  np.array(file_rec) , np.array(file_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the required statistics\n",
    "\n",
    "def cal_stat(file_rec, file_size):\n",
    "    ''' Function receivs two lists containing all file sizes and number of records respectively./\n",
    "    It then calculates all the necessary statistics.'''\n",
    "    \n",
    "    stat = {} # Dictionary to hold statistics for number of records and file sizes\n",
    "    records = {} # Dictionary to hold statistics for number of records\n",
    "    size = {} # Dictionary to hold statistics for file sizes\n",
    "    \n",
    "    for x in ['min','max','mean','25th','50th','75th','90th']:\n",
    "        if x == 'min':\n",
    "            records[x] = min(file_rec)\n",
    "            size[x] = min(file_size)\n",
    "        elif x == 'max':\n",
    "            records[x] = max(file_rec)\n",
    "            size[x] = max(file_size)\n",
    "        elif x == 'mean':\n",
    "            records[x] = np.around(np.mean(file_rec),2)\n",
    "            size[x] = np.around(np.mean(file_size),2)\n",
    "        elif x == '25th':\n",
    "            records[x] = np.around(np.percentile(file_rec,25),2)\n",
    "            size[x] = np.around(np.percentile(file_size,25),2)\n",
    "        elif x == '50th':\n",
    "            records[x] = np.around(np.percentile(file_rec,50),2)\n",
    "            size[x] = np.around(np.percentile(file_size,50),2)\n",
    "        elif x == '75th':\n",
    "            records[x] = np.around(np.percentile(file_rec,75),2)\n",
    "            size[x] = np.around(np.percentile(file_size,75),2)\n",
    "        elif x == '90th':\n",
    "            records[x] = np.around(np.percentile(file_rec,90),2)\n",
    "            size[x] = np.around(np.percentile(file_size,90),2)\n",
    "        stat['record stats'] = records\n",
    "        stat['size stats'] = size\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute basic statistics about the number of files in this sub-dataset, their size, and the number of records (lines) in each file. For length and number of records, give the min, max, mean, 25, 50, 75, 90 percentiles values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  281\n",
      "{'record stats': {'min': 15, 'max': 47703, 'mean': 17922.07, '25th': 3037.0, '50th': 19532.0, '75th': 28560.0, '90th': 31372.0}, 'size stats': {'min': 2512, 'max': 5959352, 'mean': 2152301.64, '25th': 257388.0, '50th': 1479679.0, '75th': 4181249.0, '90th': 5188938.0}}\n"
     ]
    }
   ],
   "source": [
    "#Checking the total number of files\n",
    "print(\"Total number of files: \", str(len(os.listdir(data_path))))\n",
    "\n",
    "# Retrieving values\n",
    "file_records, file_sizes = record_stat(data_path)\n",
    "\n",
    "#View statistis\n",
    "print(cal_stat(file_records,file_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the files into yellow, green, fhv, hvfhv.\n",
    "\n",
    "# Check wether folder exist if not create\n",
    "import os\n",
    "if os.path.exists('Files'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files')\n",
    "    \n",
    "for folder in ['yellow','green','FHV','FHVHV']:\n",
    "    if os.path.exists(os.path.join('Files', folder)):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(os.path.join('Files', folder))\n",
    "        \n",
    "# Reading each file and make a copy in the corresponding subfolder\n",
    "import shutil\n",
    "\n",
    "\n",
    "for file in file_content(data_path):\n",
    "    for word in ['yellow','green','FHVHV','FHV']:\n",
    "        if file.lower().startswith('yellow'):\n",
    "            my_file = data_path + file\n",
    "            shutil.copyfile(my_file,'Files/yellow/'+file)\n",
    "        elif file.lower().startswith('green'):\n",
    "            my_file = data_path + file\n",
    "            shutil.copyfile(my_file,'Files/green/'+file)\n",
    "        \n",
    "        elif file.lower().startswith('fhvhv'):\n",
    "            my_file = data_path + file\n",
    "            shutil.copyfile(my_file,'Files/FHVHV/'+file)\n",
    "        elif file.lower().startswith('fhv'):\n",
    "            my_file = data_path + file\n",
    "            shutil.copyfile(my_file,'Files/FHV/'+file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  104\n",
      "{'record stats': {'min': 959, 'max': 85995, 'mean': 26723.23, '25th': 8711.5, '50th': 24684.0, '75th': 43087.0, '90th': 47332.1}, 'size stats': {'min': 4096, 'max': 3339455, 'mean': 707580.21, '25th': 4096.0, '50th': 201559.5, '75th': 777344.75, '90th': 2742530.1}}\n"
     ]
    }
   ],
   "source": [
    "# Statistics FHV\n",
    "#Checking the total number of files\n",
    "print(\"Total number of files: \", str(len(os.listdir('Files/FHV'))))\n",
    "\n",
    "# Retrieving values\n",
    "file_records, file_sizes = record_stat('Files/FHV')\n",
    "\n",
    "#View statistis\n",
    "print(cal_stat(file_records,file_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  10\n",
      "{'record stats': {'min': 8625, 'max': 47703, 'mean': 32181.9, '25th': 18022.0, '50th': 40714.0, '75th': 43056.25, '90th': 44922.0}, 'size stats': {'min': 535789, 'max': 2978931, 'mean': 2007750.7, '25th': 1121047.25, '50th': 2542280.0, '75th': 2687857.25, '90th': 2804332.8}}\n"
     ]
    }
   ],
   "source": [
    "# Statistics FHVHV\n",
    "#Checking the total number of files\n",
    "print(\"Total number of files: \", str(len(os.listdir('./Files/FHVHV'))))\n",
    "\n",
    "# Retrieving values\n",
    "file_records, file_sizes = record_stat('./Files/FHVHV')\n",
    "\n",
    "#View statistis\n",
    "print(cal_stat(file_records,file_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  76\n",
      "{'record stats': {'min': 15, 'max': 3546, 'mean': 2026.5, '25th': 1358.75, '50th': 2072.5, '75th': 2892.25, '90th': 3126.0}, 'size stats': {'min': 2512, 'max': 570765, 'mean': 262437.29, '25th': 121494.75, '50th': 190194.5, '75th': 456955.0, '90th': 499751.0}}\n"
     ]
    }
   ],
   "source": [
    "# Statistics green\n",
    "#Checking the total number of files\n",
    "print(\"Total number of files: \", str(len(os.listdir('./Files/green'))))\n",
    "\n",
    "# Retrieving values\n",
    "file_records, file_sizes = record_stat('./Files/green')\n",
    "\n",
    "#View statistis\n",
    "print(cal_stat(file_records,file_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  131\n",
      "{'record stats': {'min': 476, 'max': 32300, 'mean': 24203.51, '25th': 19989.0, '50th': 26294.0, '75th': 29080.0, '90th': 30202.0}, 'size stats': {'min': 43103, 'max': 5959352, 'mean': 3750759.69, '25th': 1756967.0, '50th': 4442047.0, '75th': 5123591.5, '90th': 5491438.0}}\n"
     ]
    }
   ],
   "source": [
    "# Statistics yellow\n",
    "#Checking the total number of files\n",
    "print(\"Total number of files: \", str(len(os.listdir('./Files/yellow'))))\n",
    "\n",
    "# Retrieving values\n",
    "file_records, file_sizes = record_stat('./Files/yellow')\n",
    "\n",
    "#View statistis\n",
    "print(cal_stat(file_records,file_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the schema evolution.\n",
    "\n",
    "Over time, the relational schema associated to each type of trip data (yellow, green, fhv, hvfhv) has changed. Let us analyze the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to help analyze the schema changes goes here\n",
    "\n",
    "def schema(folder_ref):\n",
    "    \"\"\"Function receives a file path of a folder containing files as its argument. \n",
    "    This function checks the schema of these files and group them by their schema\"\"\"\n",
    "    \n",
    "    schema_list = [] # list to hold all unique schema\n",
    "    current_schema = [] #\n",
    "    schema_list = []\n",
    "    version = 0\n",
    "    dest_path = \"\"\n",
    "    \n",
    "    for file in file_content(folder_ref):\n",
    "        file_path = os.path.join(folder_ref, file)\n",
    "        with open(file_path) as file:  \n",
    "            schema_new = file.readline().strip().lower()\n",
    "        \n",
    "           \n",
    "        if schema_new not in schema_list:\n",
    "            schema_list.append(schema_new)\n",
    "           \n",
    "                     \n",
    "        if schema_new == current_schema:\n",
    "            \n",
    "            shutil.move(file_path,dest_path)\n",
    "        else:\n",
    "            version += 1\n",
    "            dest_path = os.path.join(folder_ref, 'v_' + str(version))\n",
    "            os.mkdir(dest_path)\n",
    "            shutil.move(file_path,dest_path)    \n",
    "            current_schema = schema_new\n",
    "            \n",
    "    return schema_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of schema changes for fhv cab data files\n",
    "\n",
    "Analyze the schema changes for the FHV cab data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "folder_ref = './Files/FHV/v_1'\n",
    "schema_FHV = schema(folder_ref)\n",
    "print(len(schema_FHV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dispatching_base_num', 'pickup_date', 'locationid'] \n",
      "\n",
      "['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid'] \n",
      "\n",
      "['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag'] \n",
      "\n",
      "['pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag', 'dispatching_base_number', 'dispatching_base_num'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in schema_FHV:\n",
    "    print(x,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files version 3 and 5 were merged because they had thesame schema\n",
    "import shutil\n",
    "import os\n",
    "    \n",
    "source_dir = './Files/FHV/v_5'\n",
    "target_dir = './Files/FHV/v_3'\n",
    "    \n",
    "file_names = os.listdir(source_dir)\n",
    "    \n",
    "for file in file_content(source_dir):\n",
    "    shutil.move(os.path.join(source_dir, file), target_dir)\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree('./Files/FHV/v_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of schema changes for fhvhv data files\n",
    "\n",
    "Analyze the schema changes for the FHV cab data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "schema_FHVHV = schema('./Files/FHVHV')\n",
    "print(len(schema_FHVHV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hvfhs_license_num', 'dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in schema_FHVHV:\n",
    "    print(x,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of schema changes for green cab data files\n",
    "\n",
    "Analyze the schema changes for the green taxi data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "schema_green = schema ('./Files/green')\n",
    "print(len(schema_green))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'total_amount', 'payment_type', 'trip_type'] \n",
      "\n",
      "['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type'] \n",
      "\n",
      "['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type'] \n",
      "\n",
      "['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in schema_green:\n",
    "    print(x,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of schema changes for yellow cab data files\n",
    "\n",
    "Analyze the schema changes for the Yellow taxi data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "schema_yellow = schema ('./Files/yellow')\n",
    "print(len(schema_yellow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vendor_name', 'trip_pickup_datetime', 'trip_dropoff_datetime', 'passenger_count', 'trip_distance', 'start_lon', 'start_lat', 'rate_code', 'store_and_forward', 'end_lon', 'end_lat', 'payment_type', 'fare_amt', 'surcharge', 'mta_tax', 'tip_amt', 'tolls_amt', 'total_amt'] \n",
      "\n",
      "['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'surcharge', 'mta_tax', 'tip_amount', 'tolls_amount', 'total_amount'] \n",
      "\n",
      "['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'ratecodeid', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount'] \n",
      "\n",
      "['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount'] \n",
      "\n",
      "['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in schema_yellow:\n",
    "    print(x,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
