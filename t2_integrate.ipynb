{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. 2\n",
    "\n",
    "Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{ASSUMPTIONS!!!.}}$ \n",
    "<br>\n",
    "$\\color{red}{\\text{You must run the t1_explore notebook before running this notebook. }}$ \n",
    "<br>\n",
    "$\\color{red}{\\text{The taxi zone folder must be in thesame location as this notebook. }}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application already started. Terminating existing application and starting new one\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#if a spark session was already started, we stop it before starting a new one\n",
    "#(there can be only one spark context per jupyter notebook)\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"TLC\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some key functions\n",
    "#generator function\n",
    "def file_content(main_folder):\n",
    "    file_list = os.listdir(main_folder)\n",
    "    for file_name in file_list:\n",
    "        yield file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "# Function to fix column names\n",
    "\n",
    "def column_name_fix(df):\n",
    "    '''Function converts column names to lowercase and remove whitespaces'''\n",
    "    colnames = df.columns\n",
    "    for x in range(len(colnames)):\n",
    "        colnames[x] =  colnames[x].strip()\n",
    "        colnames[x] =  colnames[x].lower()\n",
    "        \n",
    "    df = df.toDF(*colnames)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop column\n",
    "def column_drop(df,drop_list):\n",
    "    '''Function receives a spark dataframe and list of columns to be dropped.\n",
    "    It returns a dataframe less the columns specified to be dropped'''\n",
    "    y = []\n",
    "    for x in drop_list:\n",
    "        y.append(df.columns[x])\n",
    "    return df.drop(*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename column\n",
    "def column_rename(df,col_list1,col_list2):\n",
    "    '''Function renames the column name of a dataframe with a provided list of column names.\n",
    "        The two lists must be of the order i.e. value one in list one replaces value one in list two and so on'''\n",
    "    if len(col_list1) == len(col_list2):\n",
    "        for x in range(len(col_list1)):\n",
    "            df = df.select('*', df[col_list1[x]].alias(col_list2[x]))\n",
    "        df = column_drop(df,col_list1)\n",
    "        return df\n",
    "    else:\n",
    "        print('length of two list must be thesame.')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to a dataframe\n",
    "from pyspark.sql.functions import lit\n",
    "def column_add(df,col_list):\n",
    "    \n",
    "    for x in col_list:\n",
    "        df = df.withColumn(x, lit(\"\"))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the shapefile using geopandas (note: GeoPandas is not installed by default. If you use anaconda, you can install it by simply running conda install geopandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /usr/local/anaconda3/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: shapely in /usr/local/anaconda3/lib/python3.7/site-packages (from geopandas) (1.6.4.post2)\n",
      "Requirement already satisfied: pyproj in /usr/local/anaconda3/lib/python3.7/site-packages (from geopandas) (2.4.0)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from geopandas) (0.24.2)\n",
      "Requirement already satisfied: fiona in /usr/local/anaconda3/lib/python3.7/site-packages (from geopandas) (1.8.8)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/anaconda3/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (2.8.0)\n",
      "Requirement already satisfied: click<8,>=4.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from fiona->geopandas) (7.0)\n",
      "Requirement already satisfied: attrs>=17 in /usr/local/anaconda3/lib/python3.7/site-packages (from fiona->geopandas) (19.1.0)\n",
      "Requirement already satisfied: munch in /usr/local/anaconda3/lib/python3.7/site-packages (from fiona->geopandas) (2.3.2)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from fiona->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from fiona->geopandas) (0.5.0)\n",
      "Requirement already satisfied: six>=1.7 in /usr/local/anaconda3/lib/python3.7/site-packages (from fiona->geopandas) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('./taxi_zones/taxi_zones.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now re-project the coordinates to the CRS EPSG 4326, which is the CRS used in GPS (https://epsg.io/4326)\n",
    "zones = zones.to_crs({'init':'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygeos\n",
      "  Using cached https://files.pythonhosted.org/packages/8c/45/7dff89c1af72ed5f479a611c353768ef8309bfc1a9f5fe670ccb4be97872/pygeos-0.8-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.10 in /usr/local/anaconda3/lib/python3.7/site-packages (from pygeos) (1.16.4)\n",
      "Installing collected packages: pygeos\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/usr/local/anaconda3/lib/python3.7/site-packages/pygeos.libs'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an R-tree index on it's geometry\n",
    "\n",
    "rtree = zones.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_id(df1,zones):\n",
    "    n = len(df1)\n",
    "    for i in range(n):\n",
    "        query_point = Point( float(0 if (df1.iloc[i].pickup_longitude) is None else (df1.iloc[i].pickup_longitude)), float(0 if (df1.iloc[i].pickup_latitude) is None else (df1.iloc[i].pickup_latitude)))\n",
    "        \n",
    "        possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "       \n",
    "        for x in possible_matches:\n",
    "            if zones.iloc[x].geometry.contains(query_point)==True:\n",
    "                df1.pulocationid[i] = zones.iloc[x].LocationID\n",
    "        \n",
    "        query_point2 = Point( float(0 if (df1.iloc[i].dropoff_longitude) is None else (df1.iloc[i].dropoff_longitude)), float(0 if (df1.iloc[i].dropoff_latitude) is None else (df1.iloc[i].dropoff_latitude)))\n",
    "        possible_matches = list(rtree.intersection( query_point2.bounds ))\n",
    "        for x in possible_matches:\n",
    "            if zones.iloc[x].geometry.contains(query_point2)==True:\n",
    "                df1.dolocationid[i] = zones.iloc[x].LocationID\n",
    "    \n",
    "    return df1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a folder to hold integrated files\n",
    "# Check wether folder exist if not create\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating FHV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FHV taxi files\n",
    "\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/FHV'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV')\n",
    "\n",
    "# Schema One\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_1')\n",
    "# Columns to be renamed\n",
    "col_list1 = [1]\n",
    "col_list2 = ['pickup_datetime']\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag', 'dispatching_base_number']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = [2]\n",
    "\n",
    "folder_path = './Files/FHV/v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/FHV/v_1', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_drop(df, col_drop)\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/FHV/Schema_v_1', file),index=False)\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FHV taxi files\n",
    "# Schema Two\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_2'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_2')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['sr_flag', 'dispatching_base_number']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = []\n",
    "\n",
    "folder_path = './Files/FHV/v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/FHV/v_2', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_drop(df, col_drop)\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/FHV/Schema_v_2', file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FHV taxi files\n",
    "# Schema Three\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_3'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_3')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['dispatching_base_number']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = []\n",
    "\n",
    "folder_path = './Files/FHV/v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/FHV/v_3', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_drop(df, col_drop)\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/FHV/Schema_v_3', file),index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FHV taxi files \n",
    "# Schema Four\n",
    "import shutil\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_4'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_4')\n",
    "folder_path = './Files/FHV/v_4/'    \n",
    "# moving the files to \n",
    "for file in os.listdir(folder_path):\n",
    "    #shutil.copy2(os.path.join('Files/FHV/v_4', file), 'Files/integrated_files/FHV/Schema_v_4/')\n",
    "    my_file = folder_path + file\n",
    "    shutil.copyfile(my_file,'Files/integrated_files/FHV/Schema_v_4/'+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating FHVHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/FHVHV'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHVHV')\n",
    "\n",
    "import shutil\n",
    "if os.path.exists('Files/integrated_files/FHVHV/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHVHV/Schema_v_1')\n",
    "    \n",
    "# moving the files to \n",
    "for file in os.listdir('./Files/FHVHV/v_1'):\n",
    "    shutil.copy2(os.path.join('Files/FHVHV/v_1', file), 'Files/integrated_files/FHVHV/Schema_v_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating green files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green taxi files\n",
    "\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/green'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green')\n",
    "\n",
    "\n",
    "# schema one\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_1')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid','improvement_surcharge', 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/green/v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/green/v_1', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/green/Schema_v_1', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green taxi files\n",
    "# schema Two\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_2'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_2')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid', 'congestion_surcharge']\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/green/v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/green/v_2', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/green/Schema_v_2', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green taxi files\n",
    "# schema Three\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_3'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_3')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['congestion_surcharge']\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/green/v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/green/v_3', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/green/Schema_v_3', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Four\n",
    "import shutil\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_4'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_4')\n",
    "    \n",
    "# moving the files to \n",
    "for file in os.listdir('./Files/green/v_4'):\n",
    "    shutil.copy2(os.path.join('Files/green/v_4', file), 'Files/integrated_files/green/Schema_v_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating yellow files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow taxi files\n",
    "\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/yellow'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow')\n",
    "\n",
    "\n",
    "# schema one\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/yellow/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/Schema_v_1')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = [0,1,2,5,6,7,8,9, 10, 13]\n",
    "col_list2 = ['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','pickup_longitude', 'pickup_latitude','ratecodeid', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude','improvement_surcharge']\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid', 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/yellow/v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/yellow/v_1', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/yellow/Schema_v_1', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema Two\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/yellow/Schema_v_2'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/Schema_v_2')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = [0,1,2,7,8, 13]\n",
    "col_list2 = ['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','ratecodeid','store_and_fwd_flag','improvement_surcharge']\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid', 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "# Redefining the schema\n",
    "DDLSchema = DDLSchema = \"vendor_id String, pickup_datetime String, dropoff_datetime String,passenger_count String, trip_distance String, pickup_longitude float, pickup_latitude float, rate_code String, store_and_fwd_flag String, dropoff_longitude float, dropoff_latitude float, payment_type String, fare_amount String, surcharge String, mta_tax String, tip_amount String, tolls_amount String, total_amount String\"\n",
    "\n",
    "folder_path = './Files/yellow/v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/yellow/v_2', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path,header=True))# The read process pass via column_name_fix function\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/yellow/Schema_v_2', file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema Three\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/yellow/Schema_v_3'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/Schema_v_3')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid', 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/yellow/v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/yellow/v_3', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function   \n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/yellow/Schema_v_3', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema Four\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/yellow/Schema_v_4'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/Schema_v_4')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = [ 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = []\n",
    "\n",
    "folder_path = './Files/yellow/v_4'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/yellow/v_4', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function   \n",
    "        df = column_add(df,col_add)\n",
    "        \n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/yellow/Schema_v_4', file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Five\n",
    "import shutil\n",
    "if os.path.exists('Files/integrated_files/yellow/Schema_v_5'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/Schema_v_5')\n",
    "    \n",
    "# moving the files to \n",
    "for file in os.listdir('./Files/yellow/v_5'):\n",
    "    shutil.copy2(os.path.join('Files/yellow/v_5', file), 'Files/integrated_files/yellow/Schema_v_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
