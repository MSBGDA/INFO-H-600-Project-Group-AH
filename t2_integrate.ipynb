{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. 2\n",
    "\n",
    "Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{ASSUMPTIONS!!!.}}$ \n",
    "<br>\n",
    "$\\color{red}{\\text{You must run the t1_explore notebook before running this notebook. }}$ \n",
    "<br>\n",
    "$\\color{red}{\\text{The taxi zone folder must be in thesame location as this notebook. }}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#if a spark session was already started, we stop it before starting a new one\n",
    "#(there can be only one spark context per jupyter notebook)\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"TLC\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some key functions\n",
    "#generator function\n",
    "def file_content(main_folder):\n",
    "    file_list = os.listdir(main_folder)\n",
    "    for file_name in file_list:\n",
    "        yield file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "# Function to fix column names\n",
    "\n",
    "def column_name_fix(df):\n",
    "    '''Function converts column names to lowercase'''\n",
    "    colnames = df.columns\n",
    "    for x in range(len(colnames)):\n",
    "        colnames[x] =  colnames[x].lower()\n",
    "    df = df.toDF(*colnames)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop column\n",
    "def column_drop(df,drop_list):\n",
    "    '''Function receives a spark dataframe and list of columns to be dropped.\n",
    "    It returns a dataframe less the columns specified to be dropped'''\n",
    "    y = []\n",
    "    for x in drop_list:\n",
    "        y.append(df.columns[x])\n",
    "    return df.drop(*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename column\n",
    "def column_rename(df,col_list1,col_list2):\n",
    "    '''Function renames the column name of a dataframe with a provided list of column names.\n",
    "        The two lists must be of the order i.e. value one in list one replaces value one in list two and so on'''\n",
    "    if len(col_list1) == len(col_list2):\n",
    "        for x in range(len(col_list1)):\n",
    "            df = df.select('*', df[col_list1[x]].alias(col_list2[x]))\n",
    "        df = column_drop(df,col_list1)\n",
    "        return df\n",
    "    else:\n",
    "        print('length of two list must be thesame.')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to a dataframe\n",
    "from pyspark.sql.functions import lit\n",
    "def column_add(df,col_list):\n",
    "    \n",
    "    for x in col_list:\n",
    "        df = df.withColumn(x, lit(\"\"))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert latitude and longitude to location ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the shapefile using geopandas (note: GeoPandas is not installed by default. If you use anaconda, you can install it by simply running conda install geopandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('./taxi_zones/taxi_zones.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now re-project the coordinates to the CRS EPSG 4326, which is the CRS used in GPS (https://epsg.io/4326)\n",
    "zones = zones.to_crs({'init':'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pygeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an R-tree index on it's geometry\n",
    "\n",
    "rtree = zones.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_id(df1,zones):\n",
    "    n = len(df1)\n",
    "    for i in range(n):\n",
    "        query_point = Point( float(df1.iloc[i].pickup_longitude), float(df1.iloc[i].pickup_latitude))\n",
    "        possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "       \n",
    "        for x in possible_matches:\n",
    "            if zones.iloc[x].geometry.contains(query_point)==True:\n",
    "                df1.pulocationid[i] = zones.iloc[x].LocationID\n",
    "        \n",
    "        query_point2 = Point( float(df1.iloc[i].dropoff_longitude), float(df1.iloc[i].dropoff_latitude))\n",
    "        possible_matches = list(rtree.intersection( query_point2.bounds ))\n",
    "        for x in possible_matches:\n",
    "            if zones.iloc[x].geometry.contains(query_point2)==True:\n",
    "                df1.dolocationid[i] = zones.iloc[x].LocationID\n",
    "    \n",
    "    return df1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a folder to hold integrated files\n",
    "# Check wether folder exist if not create\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating FHV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FHV taxi files\n",
    "\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/FHV'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV')\n",
    "\n",
    "# Schema One\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_1')\n",
    "# Columns to be renamed\n",
    "col_list1 = [1]\n",
    "col_list2 = ['pickup_datetime']\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag', 'dispatching_base_number']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = [2]\n",
    "\n",
    "folder_path = './Files/FHV/v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/FHV/v_1', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_drop(df, col_drop)\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/FHV/Schema_v_1', file),index=False)\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FHV taxi files\n",
    "# Schema Two\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_2'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_2')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['sr_flag', 'dispatching_base_number']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = []\n",
    "\n",
    "folder_path = './Files/FHV/v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/FHV/v_2', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_drop(df, col_drop)\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/FHV/Schema_v_2', file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FHV taxi files\n",
    "# Schema Three\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_3'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_3')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['dispatching_base_number']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = []\n",
    "\n",
    "folder_path = './Files/FHV/v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/FHV/v_3', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_drop(df, col_drop)\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/FHV/Schema_v_3', file),index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FHV taxi files \n",
    "# Schema Four\n",
    "import shutil\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_4'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_4')\n",
    "    \n",
    "# moving the files to \n",
    "for file in os.listdir('./Files/FHV/v_4'):\n",
    "    shutil.copy2(os.path.join('Files/FHV/v_4', file), 'Files/integrated_files/FHV/Schema_v_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating green files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green taxi files\n",
    "\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/green'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green')\n",
    "\n",
    "\n",
    "# schema one\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_1')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid','improvement_surcharge', 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/green/v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/green/v_1', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/green/Schema_v_1', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green taxi files\n",
    "# schema Two\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_2'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_2')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid', 'congestion_surcharge']\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/green/v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/green/v_2', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/green/Schema_v_2', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green taxi files\n",
    "# schema Three\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_3'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_3')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['congestion_surcharge']\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/green/v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/green/v_3', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/green/Schema_v_3', file),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating yellow files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
