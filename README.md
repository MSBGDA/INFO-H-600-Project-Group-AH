
#### Authors: Kubam Ivo Mbi and Berdai Hasnae
## 1.0. Introduction
The New York City Taxi and Limousine Commission (TLC) was created in 1971. This agency is responsible for licensing and regulating New York City's Medallion (Yellow) taxi cabs, for-hire vehicles (community-based liveries, black cars and luxury limousines), commuter vans, and paratransit vehicles opereate in the city. It is approximated that about one million trips are recorded each day. There are Four types of trips broken down into Yellow Taxi, Green Taxi, For-Hire Vehicle (FHV) and High Volume For-Hire Vehicle (FHVHV). TLC receives taxi trip data from the technology service providers (TSPs) that provide electronic metering in each cab, and FHV trip data from the app, community livery, black car, or luxury limousine company, or base, who dispatched the trip. Visit [About TLC](https://www1.nyc.gov/site/tlc/about/about-tlc.page) for more info. 
## 2.0. Project Objective
This project was aimed at using the tools and knowledge acquired in the course to do data wrangling and analysis of a sampled subset of data from the TLC database. This objective was broken down into the following tasks:<br>
> - **2.1. Collecting metadata, inspecting schema evolution:** Understanding the data and its characteristics
> - **2.2. Data integration:** Updating old schemas to the latest schema for each subdatset
> - **2.3. Data cleaning:** Checking files for record errors, repairing them and separting into good and bad records.
> - **2.4. Analysis:** Answering queries and plotting results using matplotlib

<br>See the assignment file for more details about the above objectives. 
## 3.0. Methodology
### 3.1. About the Dataset
The dataset used in this project was generated by uniformly sampling 0.2\% without repetition from each file per taxi trip types. The sampled dataset consist of records from 2009 with just the Yellow taxi uptil 2020 with all four taxi records included. The four sub-datasets representing the four trip types do not provide the same information. More information is provided by the data dictionaries found on the [TLC Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) 
### 3.2. Solution Approach
Functional programming was the backbone to our solution approach. We will break the solution approach per project tasks. <br>
> - **Collecting metadata, inspecting schema evolution:** To compute the basic statistics, we utilized two main functions. The first function (***record_stat***) receives a file path containing files and record each file size and its number of records both saved in two separate lists. The returned lists is passed onto the function (***cal_stat***) which then calculates the requested statistics per taxi type. To analyze the schema, a function called ***schema***  receives the folder path containing files, gets the schema of each file, compares them and separates files based on schema similarities or differences. This was done per taxi subtypes. 

> - **Data integration:** We made use of five main functions. ***column_name_fix*** to lowercase and remove all whitespace in column names. ***column_drop*** to drop selected columns from a dataframe.***column_rename*** to rename selected columns with a provided list of column names. ***column_add*** to add list of columns to a dataframe. <br>In order to convert longitudes and latitudes to locationID we use the function ***location_id***. This function receives a dataframe and zones as its parameters. The zones were read from taxi_zones.shp file and then re-projected to the CRS EPSG 4326 system. From zones, rtree index was created. When a dataframe is pass into the ***location_id*** function, a query point is generated from the pickup longitude and and latitude and same for the dropoff longitude and latitude. These query points are intersected with the rtree for possible matches. These possible matches are check in zones geometry if there exist a value and that value is attributed pickup locationID or dropoff locationID respectively.

> - **Data Cleaning** To clean the integrated datasets, we started by adding a new column called dirty to each file using the ***add_column*** function. Other functions used to check the cleanliness of the dataset were ***null_value*** to check columns for null value, ***locationID*** to make sure all locationID values were between 1 and 265. Passing each file through the above mention functions, records that did not pass the criteria were flagged as dirty. In order to repair the flagged records, we used the function ***repair_missing*** which replaces missing values of selected columns by a defined value. The ***repair_absolute*** function was used on selected numeric fields to make sure there were positive. Finally, ***record_separation*** function was used to separate good records from bad ones. All cleaned files per subdataset was stacked into one. So the final output from data cleaning had four files. One each for FHV, FHVHV, green and yellow.  <br>
**Note should be taken that we only considered columns that were required for the analysis section**

>- **Analysis:** Before hand we had extracted trip month and total receipt for each file in the data cleaning section using the ***month_extract*** and ***tot_receipt*** function respectively. So it was then easy for us to group most of the analysis by month as requested. Trip duration in minutes was extracted from the difference in pickup and dropoff time fields. The trip speed was calcualted diving the trip distance covered in miles by trip duration in minutes. 
