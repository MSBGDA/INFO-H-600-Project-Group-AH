{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some key functions\n",
    "#generator function\n",
    "def file_content(main_folder):\n",
    "    file_list = sorted(os.listdir(main_folder))\n",
    "    for file_name in file_list:\n",
    "        if os.path.isfile(os.path.join(main_folder, file_name)): # output only files and no subfolders\n",
    "            yield file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add indicator column\n",
    "def add_col(df):\n",
    "    df['dirty']= 0\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def null_value(df, col):\n",
    "    n = df.shape[0]\n",
    "    for x in range(n):  \n",
    "        if df.loc[x,col].isna().any():\n",
    "            df.loc[x, 'dirty']= 1  \n",
    "        else:\n",
    "            df.loc[x, 'dirty']= 0\n",
    "                \n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting month from date field, calculating total amount, trip duration\n",
    "def extract_month(df, field):\n",
    "    df['trip_month'] = pd.to_datetime(df[field]).dt.month\n",
    "    return df\n",
    "\n",
    "def tot_amount(df,col_list):\n",
    "    df['tot_amount'] = df[col_list].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "def trip_duration(df,col_pickup,col_dropoff):\n",
    "    df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).astype('timedelta64[m]')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check location values\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def locationid(df,col):\n",
    "    n = df.shape[0]\n",
    "    for x in range(n):\n",
    "        if df.loc[x, 'dirty'] != 1:\n",
    "            if (1 <= df.loc[x,col[0]] <= 265) != True or (1 <= df.loc[x,col[1]] <= 265) != True:\n",
    "                df.loc[x, 'dirty']= 1\n",
    "    return df\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repairing missing values for numerical dimensions\n",
    "def repair_missing(df,col,value):\n",
    "    df[col] = df[col].fillna(value) \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparing negative numeric dimensions\n",
    "def repair_absolute(df,col):\n",
    "    df[col] = df[col].abs()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting rows with pickup time less than dropoff time\n",
    "def check_time(df):\n",
    "    index_name = df[df['trip_duration']<0].index\n",
    "    df.drop(index_name,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all numeric dimensions to float type\n",
    "def type_conversion(df, col, dtype):\n",
    "    if not int(df[col]):\n",
    "        \n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating clean and bad records\n",
    "\n",
    "def record_separation(df):\n",
    "    df\n",
    "    return  df.loc[df['dirty'] == 0], df.loc[df['dirty'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning the FHV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether folder exist if not create\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/clean'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/clean')\n",
    "    \n",
    "if os.path.exists('Files/integrated_files/FHV/dirty'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/dirty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCHEMA VERSION 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "col_check = ['pickup_datetime', 'dropoff_datetime', 'pulocationid','dolocationid']\n",
    "col_order = ['pickup_datetime','dropoff_datetime','pulocationid','dolocationid','sr_flag','dispatching_base_num']\n",
    "field = 'pickup_datetime'\n",
    "folder_path = './Files/integrated_files/FHV/Schema_v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/FHV/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/FHV/dirty', file),index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCHEMA VERSION 2\n",
    "\n",
    "import pandas as pd\n",
    "col_check = ['pickup_datetime', 'dropoff_datetime', 'pulocationid','dolocationid']\n",
    "col_order = ['pickup_datetime','dropoff_datetime','pulocationid','dolocationid','sr_flag','dispatching_base_num']\n",
    "field = 'pickup_datetime'\n",
    "folder_path = './Files/integrated_files/FHV/Schema_v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/FHV/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/FHV/dirty', file),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCHEMA VERSION 3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "col_check = ['pickup_datetime', 'dropoff_datetime', 'pulocationid','dolocationid']\n",
    "col_abs = ['pulocationid','dolocationid']\n",
    "col_order = ['pickup_datetime','dropoff_datetime','pulocationid','dolocationid','sr_flag','dispatching_base_num']\n",
    "field = 'pickup_datetime'\n",
    "\n",
    "folder_path = './Files/integrated_files/FHV/Schema_v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = null_value(df,col_check)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/FHV/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/FHV/dirty', file),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCHEMA VERSION 4\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "col_check = ['pickup_datetime', 'dropoff_datetime', 'pulocationid','dolocationid']\n",
    "col_abs = ['pulocationid','dolocationid']\n",
    "col_order = ['pickup_datetime','dropoff_datetime','pulocationid','dolocationid','sr_flag','dispatching_base_num']\n",
    "field = 'pickup_datetime'\n",
    "\n",
    "folder_path = './Files/integrated_files/FHV/Schema_v_4'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = null_value(df,col_check)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/FHV/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/FHV/dirty', file),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667814\n"
     ]
    }
   ],
   "source": [
    "#Statistics for bad records\n",
    "x = 0\n",
    "folder_path = 'Files/integrated_files/FHV/dirty'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        x += df.shape[0]\n",
    "print(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking all files into one per dataset type\n",
    "\n",
    "df_list = []\n",
    "folder_path = 'Files/integrated_files/FHV/clean'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "df_fhv = pd.concat(df_list)\n",
    "df_fhv.to_csv('df_fhv.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the FHVHV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/FHVHV/clean'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHVHV/clean')\n",
    "    \n",
    "if os.path.exists('Files/integrated_files/FHVHV/dirty'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHVHV/dirty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_check = ['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID']\n",
    "col_abs = ['PULocationID','DOLocationID']\n",
    "field = 'pickup_datetime'\n",
    "\n",
    "folder_path = './Files/integrated_files/FHVHV/Schema_v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = null_value(df,col_check)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_abs)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/FHVHV/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/FHVHV/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Statistics for bad records\n",
    "x = 0\n",
    "folder_path = 'Files/integrated_files/FHVHV/dirty'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        x += df.shape[0]\n",
    "print(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking all files into one per dataset type\n",
    "\n",
    "df_list = []\n",
    "folder_path = 'Files/integrated_files/FHVHV/clean'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "df_fhvhv = pd.concat(df_list)\n",
    "df_fhvhv.to_csv('df_fhvhv.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning the GREEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/green/clean'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/clean')\n",
    "    \n",
    "if os.path.exists('Files/integrated_files/green/dirty'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/dirty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 1\n",
    "\n",
    "col_check = ['lpep_pickup_datetime','lpep_dropoff_datetime','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "col_order = ['vendorid','lpep_pickup_datetime','lpep_dropoff_datetime','store_and_fwd_flag','ratecodeid','passenger_count','trip_distance','fare_amount','extra','mta_tax','tip_amount','tolls_amount','ehail_fee','total_amount','payment_type','trip_type','pulocationid','dolocationid','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "field = 'lpep_pickup_datetime'\n",
    "\n",
    "col_list = ['fare_amount','extra','mta_tax','tolls_amount','ehail_fee','total_amount', 'improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "folder_path = './Files/integrated_files/green/Schema_v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :                       \n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean = tot_amount(df_clean,col_list)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/green/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/green/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 2\n",
    "\n",
    "col_check = ['lpep_pickup_datetime','lpep_dropoff_datetime','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "col_order = ['vendorid','lpep_pickup_datetime','lpep_dropoff_datetime','store_and_fwd_flag','ratecodeid','passenger_count','trip_distance','fare_amount','extra','mta_tax','tip_amount','tolls_amount','ehail_fee','total_amount','payment_type','trip_type','pulocationid','dolocationid','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "field = 'lpep_pickup_datetime'\n",
    "\n",
    "col_list = ['fare_amount','extra','mta_tax','tolls_amount','ehail_fee','total_amount', 'improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "folder_path = './Files/integrated_files/green/Schema_v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :            \n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean = tot_amount(df_clean,col_list)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/green/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/green/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 3\n",
    "\n",
    "col_check = ['lpep_pickup_datetime','lpep_dropoff_datetime','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "col_order = ['vendorid','lpep_pickup_datetime','lpep_dropoff_datetime','store_and_fwd_flag','ratecodeid','passenger_count','trip_distance','fare_amount','extra','mta_tax','tip_amount','tolls_amount','ehail_fee','total_amount','payment_type','trip_type','pulocationid','dolocationid','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "field = 'lpep_pickup_datetime'\n",
    "\n",
    "col_list = ['fare_amount','extra','mta_tax','tolls_amount','ehail_fee','total_amount', 'improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "\n",
    "folder_path = './Files/integrated_files/green/Schema_v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :            \n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean = tot_amount(df_clean,col_list)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/green/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/green/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 4\n",
    "\n",
    "col_check = ['lpep_pickup_datetime','lpep_dropoff_datetime','trip_distance','fare_amount','mta_tax','PULocationID','DOLocationID','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['PULocationID','DOLocationID','trip_distance','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['PULocationID','DOLocationID']\n",
    "\n",
    "col_order = ['vendorid','lpep_pickup_datetime','lpep_dropoff_datetime','store_and_fwd_flag','ratecodeid','passenger_count','trip_distance','fare_amount','extra','mta_tax','tip_amount','tolls_amount','ehail_fee','total_amount','payment_type','trip_type','pulocationid','dolocationid','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "field = 'lpep_pickup_datetime'\n",
    "\n",
    "col_list = ['fare_amount','extra','mta_tax','tolls_amount','ehail_fee','total_amount', 'improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "folder_path = './Files/integrated_files/green/Schema_v_4'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean = df_clean.rename(columns={'PULocationID':'pulocationid','DOLocationID':'dolocationid','VendorID':'vendorid', 'RatecodeID':'ratecodeid'})\n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean = tot_amount(df_clean,col_list)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/green/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/green/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526\n"
     ]
    }
   ],
   "source": [
    "#Statistics for bad records\n",
    "x = 0\n",
    "folder_path = 'Files/integrated_files/green/dirty'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        x += df.shape[0]\n",
    "print(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking all files into one per dataset type\n",
    "\n",
    "df_list = []\n",
    "folder_path = 'Files/integrated_files/green/clean'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "df_green = pd.concat(df_list)\n",
    "df_green.to_csv('df_green.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the YELLOW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/yellow/clean'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/clean')\n",
    "    \n",
    "if os.path.exists('Files/integrated_files/yellow/dirty'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/dirty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 1\n",
    "\n",
    "col_check = ['tpep_pickup_datetime','tpep_dropoff_datetime','trip_distance','fare_amt','mta_tax','pulocationid','dolocationid','tolls_amt','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','fare_amt','mta_tax','tolls_amt','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amt','congestion_surcharge']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "col_list = ['fare_amount','mta_tax','tolls_amount','total_amount', 'improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_order = ['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount','congestion_surcharge']\n",
    "field = 'tpep_pickup_datetime'\n",
    "\n",
    "\n",
    "folder_path = './Files/integrated_files/yellow/Schema_v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean = df_clean.rename(columns={'fare_amt':'fare_amount','total_amt':'total_amount','tolls_amt':'tolls_amount', 'tip_amt':'tip_amount'})\n",
    "            df_clean['extra'] = 0\n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean = tot_amount(df_clean,col_list)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/yellow/clean', file),index=False)\n",
    "        #if df_dirty.shape[0] > 0 :\n",
    "         #   df_dirty.to_csv(os.path.join('Files/integrated_files/yellow/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## Schema 2\n",
    "\n",
    "col_check = ['tpep_pickup_datetime','tpep_dropoff_datetime','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "col_float = ['trip_distance','mta_tax','pulocationid','dolocationid','tolls_amount']\n",
    "\n",
    "col_list = ['fare_amount','extra','mta_tax','tolls_amount','total_amount', 'improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_order = ['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount','congestion_surcharge']\n",
    "field = 'tpep_pickup_datetime'\n",
    "\n",
    "\n",
    "folder_path = './Files/integrated_files/yellow/Schema_v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)       \n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        #df.drop([152,2378,8406,11208,11351,11661,13045,13898,14575,15039,15842], inplace=True) # dropping Rows with string values\n",
    "        #df = type_conversion(df, col_float, float)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean['extra'] = 0\n",
    "            df_clean['store_and_fwd_flag'] = ''\n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean = tot_amount(df_clean,col_list)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/yellow/clean', file),index=False)\n",
    "        #if df_dirty.shape[0] > 0 :\n",
    "        #    df_dirty.to_csv(os.path.join('Files/integrated_files/yellow/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 3\n",
    "\n",
    "col_check = ['tpep_pickup_datetime','tpep_dropoff_datetime','extra','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','extra','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge','extra']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "col_list = ['fare_amount','extra','mta_tax','tolls_amount','total_amount', 'improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_order = ['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount','congestion_surcharge']\n",
    "field = 'tpep_pickup_datetime'\n",
    "\n",
    "\n",
    "folder_path = './Files/integrated_files/yellow/Schema_v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean = tot_amount(df_clean,col_list)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/yellow/clean', file),index=False)\n",
    "        #if df_dirty.shape[0] > 0 :\n",
    "        #    df_dirty.to_csv(os.path.join('Files/integrated_files/yellow/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 4\n",
    "\n",
    "col_check = ['tpep_pickup_datetime','tpep_dropoff_datetime','extra','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','extra','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge','extra']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "col_list = ['fare_amount','extra','mta_tax','tolls_amount','total_amount', 'improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_order = ['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount','congestion_surcharge']\n",
    "field = 'tpep_pickup_datetime'\n",
    "\n",
    "\n",
    "folder_path = './Files/integrated_files/yellow/Schema_v_4'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean = tot_amount(df_clean,col_list)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/yellow/clean', file),index=False)\n",
    "        #if df_dirty.shape[0] > 0 :\n",
    "        #   df_dirty.to_csv(os.path.join('Files/integrated_files/yellow/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 5\n",
    "\n",
    "col_check = ['tpep_pickup_datetime','tpep_dropoff_datetime','extra','trip_distance','fare_amount','mta_tax','PULocationID','DOLocationID','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['PULocationID','DOLocationID','trip_distance','extra','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge','extra']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['PULocationID','DOLocationID']\n",
    "\n",
    "col_list = ['fare_amount','extra','mta_tax','tolls_amount','total_amount', 'improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_order = ['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount','congestion_surcharge']\n",
    "field = 'tpep_pickup_datetime'\n",
    "\n",
    "\n",
    "folder_path = './Files/integrated_files/yellow/Schema_v_5'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean = df_clean.rename(columns={'PULocationID':'pulocationid','DOLocationID':'dolocationid','VendorID':'vendorid', 'RatecodeID':'ratecodeid'})\n",
    "            df_clean =df_clean[col_order]\n",
    "            df_clean = extract_month(df_clean,field)\n",
    "            df_clean = tot_amount(df_clean,col_list)\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/yellow/clean', file),index=False)\n",
    "        #if df_dirty.shape[0] > 0 :\n",
    "        #    df_dirty.to_csv(os.path.join('Files/integrated_files/yellow/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341112\n"
     ]
    }
   ],
   "source": [
    "#Statistics for bad records\n",
    "x = 0\n",
    "folder_path = 'Files/integrated_files/yellow/dirty'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        x += df.shape[0]\n",
    "print(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking all files into one per dataset type\n",
    "\n",
    "df_list = []\n",
    "folder_path = 'Files/integrated_files/yellow/clean'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "df_yellow = pd.concat(df_list)\n",
    "df_yellow.to_csv('df_yellow.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/epb103/yellow.zip'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('yellow', 'zip', \"Files/integrated_files/yellow/clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
