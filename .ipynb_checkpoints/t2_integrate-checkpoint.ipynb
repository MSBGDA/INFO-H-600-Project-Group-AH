{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. 2\n",
    "\n",
    "Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{ASSUMPTIONS!!!.}}$ \n",
    "<br>\n",
    "$\\color{red}{\\text{You must run the t1_explore notebook before running this notebook. }}$ \n",
    "<br>\n",
    "$\\color{red}{\\text{The taxi zone folder must be in thesame location as this notebook. }}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#if a spark session was already started, we stop it before starting a new one\n",
    "#(there can be only one spark context per jupyter notebook)\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"TLC\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some key functions\n",
    "#generator function\n",
    "def file_content(main_folder):\n",
    "    file_list = os.listdir(main_folder)\n",
    "    for file_name in file_list:\n",
    "        yield file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "# Function to fix column names\n",
    "\n",
    "def column_name_fix(df):\n",
    "    '''Function converts column names to lowercase and remove whitespaces'''\n",
    "    colnames = df.columns\n",
    "    for x in range(len(colnames)):\n",
    "        colnames[x] =  colnames[x].strip()\n",
    "        colnames[x] =  colnames[x].lower()\n",
    "        \n",
    "    df = df.toDF(*colnames)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+----------------+------------------+-------------------+------------------+----------+-------------------+-------------------+------------------+-------------+------------+----------+--------+-----------+-------------+-------------+\n",
      "|      _c0|                _c1|                _c2|             _c3|               _c4|                _c5|               _c6|       _c7|                _c8|                _c9|              _c10|         _c11|        _c12|      _c13|    _c14|       _c15|         _c16|         _c17|\n",
      "+---------+-------------------+-------------------+----------------+------------------+-------------------+------------------+----------+-------------------+-------------------+------------------+-------------+------------+----------+--------+-----------+-------------+-------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime| passenger_count|     trip_distance|   pickup_longitude|   pickup_latitude| rate_code| store_and_fwd_flag|  dropoff_longitude|  dropoff_latitude| payment_type| fare_amount| surcharge| mta_tax| tip_amount| tolls_amount| total_amount|\n",
      "|      VTS|2014-01-07 22:51:00|2014-01-07 22:55:00|               6|               1.2|         -73.995542|40.763817000000003|         1|               null|-73.988595000000004|40.778241999999999|          CRD|           6|       0.5|     0.5|          1|            0|            8|\n",
      "|      VTS|2014-01-22 19:29:00|2014-01-22 19:37:00|               1|1.4399999999999999|-73.976061999999999|40.752082000000001|         1|               null|-73.986446999999998|40.734594999999999|          CRD|         7.5|         1|     0.5|          1|            0|           10|\n",
      "|      CMT|2014-01-09 22:18:07|2014-01-09 22:23:01|               4|               1.2|-73.984499999999997|40.748128999999999|         1|                  N|-73.983644999999996|40.758766000000001|          CSH|           6|       0.5|     0.5|          0|            0|            7|\n",
      "|      CMT|2014-01-05 00:03:18|2014-01-05 00:16:00|               1|               1.3|-74.005705000000006|40.743012999999998|         1|                  N|-73.988865000000004|40.731340000000003|          CRD|           9|       0.5|     0.5|          2|            0|           12|\n",
      "+---------+-------------------+-------------------+----------------+------------------+-------------------+------------------+----------+-------------------+-------------------+------------------+-------------+------------+----------+--------+-----------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = column_name_fix(spark.read.csv('./Files/yellow/v_2/yellow_tripdata_2014-01.csv'))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop column\n",
    "def column_drop(df,drop_list):\n",
    "    '''Function receives a spark dataframe and list of columns to be dropped.\n",
    "    It returns a dataframe less the columns specified to be dropped'''\n",
    "    y = []\n",
    "    for x in drop_list:\n",
    "        y.append(df.columns[x])\n",
    "    return df.drop(*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename column\n",
    "def column_rename(df,col_list1,col_list2):\n",
    "    '''Function renames the column name of a dataframe with a provided list of column names.\n",
    "        The two lists must be of the order i.e. value one in list one replaces value one in list two and so on'''\n",
    "    if len(col_list1) == len(col_list2):\n",
    "        for x in range(len(col_list1)):\n",
    "            df = df.select('*', df[col_list1[x]].alias(col_list2[x]))\n",
    "        df = column_drop(df,col_list1)\n",
    "        return df\n",
    "    else:\n",
    "        print('length of two list must be thesame.')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to a dataframe\n",
    "from pyspark.sql.functions import lit\n",
    "def column_add(df,col_list):\n",
    "    \n",
    "    for x in col_list:\n",
    "        df = df.withColumn(x, lit(\"\"))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the shapefile using geopandas (note: GeoPandas is not installed by default. If you use anaconda, you can install it by simply running conda install geopandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /opt/conda/lib/python3.8/site-packages (0.8.1)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from geopandas) (1.1.4)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from geopandas) (3.0.0.post1)\n",
      "Requirement already satisfied: fiona in /opt/conda/lib/python3.8/site-packages (from geopandas) (1.8.18)\n",
      "Requirement already satisfied: shapely in /opt/conda/lib/python3.8/site-packages (from geopandas) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23.0->geopandas) (2020.4)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23.0->geopandas) (1.19.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from pyproj>=2.2.0->geopandas) (2020.11.8)\n",
      "Requirement already satisfied: attrs>=17 in /opt/conda/lib/python3.8/site-packages (from fiona->geopandas) (20.3.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /opt/conda/lib/python3.8/site-packages (from fiona->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.8/site-packages (from fiona->geopandas) (0.7.1)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.8/site-packages (from fiona->geopandas) (1.15.0)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.8/site-packages (from fiona->geopandas) (2.5.0)\n",
      "Requirement already satisfied: click<8,>=4.0 in /opt/conda/lib/python3.8/site-packages (from fiona->geopandas) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/geopandas/_compat.py:84: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.8.1-CAPI-1.13.3). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('./taxi_zones/taxi_zones.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "# Now re-project the coordinates to the CRS EPSG 4326, which is the CRS used in GPS (https://epsg.io/4326)\n",
    "zones = zones.to_crs({'init':'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygeos in /opt/conda/lib/python3.8/site-packages (0.8)\n",
      "Requirement already satisfied: numpy>=1.10 in /opt/conda/lib/python3.8/site-packages (from pygeos) (1.19.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an R-tree index on it's geometry\n",
    "\n",
    "rtree = zones.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_id(df1,zones):\n",
    "    n = len(df1)\n",
    "    for i in range(n):\n",
    "        query_point = Point( float(0 if (df1.iloc[i].pickup_longitude) is None else (df1.iloc[i].pickup_longitude)), float(0 if (df1.iloc[i].pickup_latitude) is None else (df1.iloc[i].pickup_latitude)))\n",
    "        \n",
    "        possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "       \n",
    "        for x in possible_matches:\n",
    "            if zones.iloc[x].geometry.contains(query_point)==True:\n",
    "                df1.pulocationid[i] = zones.iloc[x].LocationID\n",
    "        \n",
    "        query_point2 = Point( float(0 if (df1.iloc[i].dropoff_longitude) is None else (df1.iloc[i].dropoff_longitude)), float(0 if (df1.iloc[i].dropoff_latitude) is None else (df1.iloc[i].dropoff_latitude)))\n",
    "        possible_matches = list(rtree.intersection( query_point2.bounds ))\n",
    "        for x in possible_matches:\n",
    "            if zones.iloc[x].geometry.contains(query_point2)==True:\n",
    "                df1.dolocationid[i] = zones.iloc[x].LocationID\n",
    "    \n",
    "    return df1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a folder to hold integrated files\n",
    "# Check wether folder exist if not create\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating FHV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FHV taxi files\n",
    "\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/FHV'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV')\n",
    "\n",
    "# Schema One\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_1')\n",
    "# Columns to be renamed\n",
    "col_list1 = [1]\n",
    "col_list2 = ['pickup_datetime']\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag', 'dispatching_base_number']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = [2]\n",
    "\n",
    "folder_path = './Files/FHV/v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/FHV/v_1', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_drop(df, col_drop)\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/FHV/Schema_v_1', file),index=False)\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FHV taxi files\n",
    "# Schema Two\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_2'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_2')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['sr_flag', 'dispatching_base_number']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = []\n",
    "\n",
    "folder_path = './Files/FHV/v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/FHV/v_2', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_drop(df, col_drop)\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/FHV/Schema_v_2', file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FHV taxi files\n",
    "# Schema Three\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_3'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_3')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['dispatching_base_number']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = []\n",
    "\n",
    "folder_path = './Files/FHV/v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/FHV/v_3', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_drop(df, col_drop)\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/FHV/Schema_v_3', file),index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FHV taxi files \n",
    "# Schema Four\n",
    "import shutil\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_4'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_4')\n",
    "    \n",
    "# moving the files to \n",
    "for file in os.listdir('./Files/FHV/v_4'):\n",
    "    shutil.copy2(os.path.join('Files/FHV/v_4', file), 'Files/integrated_files/FHV/Schema_v_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating FHVHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/FHVHV'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHVHV')\n",
    "\n",
    "import shutil\n",
    "if os.path.exists('Files/integrated_files/FHVHV/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHVHV/Schema_v_1')\n",
    "    \n",
    "# moving the files to \n",
    "for file in os.listdir('./Files/FHVHV/v_1'):\n",
    "    shutil.copy2(os.path.join('Files/FHVHV/v_1', file), 'Files/integrated_files/FHVHV/Schema_v_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating green files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green taxi files\n",
    "\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/green'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green')\n",
    "\n",
    "\n",
    "# schema one\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_1')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid','improvement_surcharge', 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/green/v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/green/v_1', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/green/Schema_v_1', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green taxi files\n",
    "# schema Two\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_2'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_2')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid', 'congestion_surcharge']\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/green/v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/green/v_2', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/green/Schema_v_2', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green taxi files\n",
    "# schema Three\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_3'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_3')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['congestion_surcharge']\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/green/v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/green/v_3', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_add(df,col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/green/Schema_v_3', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Four\n",
    "import shutil\n",
    "if os.path.exists('Files/integrated_files/green/Schema_v_4'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/Schema_v_4')\n",
    "    \n",
    "# moving the files to \n",
    "for file in os.listdir('./Files/green/v_4'):\n",
    "    shutil.copy2(os.path.join('Files/green/v_4', file), 'Files/integrated_files/green/Schema_v_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating yellow files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow taxi files\n",
    "\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/yellow'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow')\n",
    "\n",
    "\n",
    "# schema one\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/yellow/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/Schema_v_1')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = [0,1,2,5,6,7,8,9, 10, 13]\n",
    "col_list2 = ['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','pickup_longitude', 'pickup_latitude','ratecodeid', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude','improvement_surcharge']\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid', 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/yellow/v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/yellow/v_1', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/yellow/Schema_v_1', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema Two\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/yellow/Schema_v_2'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/Schema_v_2')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = [0,1,2,7,8, 13]\n",
    "col_list2 = ['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','ratecodeid','store_and_fwd_flag','improvement_surcharge']\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid', 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "# Redefining the schema\n",
    "DDLSchema = DDLSchema = \"vendor_id String, pickup_datetime String, dropoff_datetime String,passenger_count String, trip_distance String, pickup_longitude float, pickup_latitude float, rate_code String, store_and_fwd_flag String, dropoff_longitude float, dropoff_latitude float, payment_type String, fare_amount String, surcharge String, mta_tax String, tip_amount String, tolls_amount String, total_amount String\"\n",
    "\n",
    "folder_path = './Files/yellow/v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/yellow/v_2', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path,header=True))# The read process pass via column_name_fix function\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/yellow/Schema_v_2', file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema Three\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/yellow/Schema_v_3'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/Schema_v_3')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = ['pulocationid', 'dolocationid', 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "folder_path = './Files/yellow/v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/yellow/v_3', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function   \n",
    "        df = column_add(df,col_add)\n",
    "        df = df.toPandas()\n",
    "        df = location_id(df,zones)\n",
    "        df = df.drop(columns= col_drop)\n",
    "        df.to_csv(os.path.join('Files/integrated_files/yellow/Schema_v_3', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema Four\n",
    "\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/yellow/Schema_v_4'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/Schema_v_4')\n",
    "    \n",
    "# Columns to be renamed\n",
    "col_list1 = []\n",
    "col_list2 = []\n",
    "\n",
    "# columns to be added\n",
    "col_add = [ 'congestion_surcharge']\n",
    "\n",
    "# columnns to be dropped\n",
    "col_drop = []\n",
    "\n",
    "folder_path = './Files/yellow/v_4'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/yellow/v_4', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function   \n",
    "        df = column_add(df,col_add)\n",
    "        \n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/yellow/Schema_v_4', file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Five\n",
    "import shutil\n",
    "if os.path.exists('Files/integrated_files/yellow/Schema_v_5'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/Schema_v_5')\n",
    "    \n",
    "# moving the files to \n",
    "for file in os.listdir('./Files/yellow/v_5'):\n",
    "    shutil.copy2(os.path.join('Files/yellow/v_5', file), 'Files/integrated_files/yellow/Schema_v_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
