{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some key functions\n",
    "#generator function\n",
    "def file_content(main_folder):\n",
    "    file_list = sorted(os.listdir(main_folder))\n",
    "    for file_name in file_list:\n",
    "        if os.path.isfile(os.path.join(main_folder, file_name)): # output only files and no subfolders\n",
    "            yield file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add indicator column\n",
    "def add_col(df):\n",
    "    df['dirty']= 0\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def null_value(df, col):\n",
    "    n = df.shape[0]\n",
    "    for x in range(n):  \n",
    "        if df.loc[x,col].isna().any():\n",
    "            df.loc[x, 'dirty']= 1  \n",
    "        else:\n",
    "            df.loc[x, 'dirty']= 0\n",
    "                \n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check location values\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def locationid(df,col):\n",
    "    n = df.shape[0]\n",
    "    for x in range(n):\n",
    "        if df.loc[x, 'dirty'] != 1:\n",
    "            if (1 <= df.loc[x,col[0]] <= 265) != True or (1 <= df.loc[x,col[1]] <= 265) != True:\n",
    "                df.loc[x, 'dirty']= 1\n",
    "    return df\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repairing missing values for numerical dimensions\n",
    "def repair_missing(df,col,value):\n",
    "    df[col] = df[col].fillna(value) \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparing negative numeric dimensions\n",
    "def repair_absolute(df,col):\n",
    "    df[col] = df[col].abs()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all numeric dimensions to float type\n",
    "def type_conversion(df, col, dtype):\n",
    "    if not int(df[col]):\n",
    "        \n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating clean and bad records\n",
    "\n",
    "def record_separation(df):\n",
    "    df\n",
    "    return  df.loc[df['dirty'] == 0], df.loc[df['dirty'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning the FHV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/FHV/clean'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/clean')\n",
    "    \n",
    "if os.path.exists('Files/integrated_files/FHV/dirty'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/dirty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_check = ['pickup_datetime', 'dropoff_datetime', 'pulocationid','dolocationid']\n",
    "\n",
    "folder_path = './Files/integrated_files/FHV/Schema_v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/FHV/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/FHV/dirty', file),index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "col_check = ['pickup_datetime', 'dropoff_datetime', 'pulocationid','dolocationid']\n",
    "\n",
    "folder_path = './Files/integrated_files/FHV/Schema_v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/FHV/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/FHV/dirty', file),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_check = ['pickup_datetime', 'dropoff_datetime', 'pulocationid','dolocationid']\n",
    "col_abs = ['pulocationid','dolocationid']\n",
    "\n",
    "folder_path = './Files/integrated_files/FHV/Schema_v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = null_value(df,col_check)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/FHV/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/FHV/dirty', file),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_check = ['pickup_datetime', 'dropoff_datetime', 'pulocationid','dolocationid']\n",
    "col_abs = ['pulocationid','dolocationid']\n",
    "\n",
    "folder_path = './Files/integrated_files/FHV/Schema_v_4'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = null_value(df,col_check)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/FHV/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/FHV/dirty', file),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667814\n"
     ]
    }
   ],
   "source": [
    "#Statistics for bad records\n",
    "x = 0\n",
    "folder_path = 'Files/integrated_files/FHV/dirty'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        x += df.shape[0]\n",
    "print(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the FHVHV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/FHVHV/clean'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHVHV/clean')\n",
    "    \n",
    "if os.path.exists('Files/integrated_files/FHVHV/dirty'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHVHV/dirty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_check = ['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID']\n",
    "col_abs = ['PULocationID','DOLocationID']\n",
    "\n",
    "folder_path = './Files/integrated_files/FHVHV/Schema_v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = null_value(df,col_check)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_abs)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/FHVHV/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/FHVHV/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Statistics for bad records\n",
    "x = 0\n",
    "folder_path = 'Files/integrated_files/FHVHV/dirty'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        x += df.shape[0]\n",
    "print(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning the GREEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/green/clean'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/clean')\n",
    "    \n",
    "if os.path.exists('Files/integrated_files/green/dirty'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/green/dirty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 1\n",
    "\n",
    "col_check = ['lpep_pickup_datetime','lpep_dropoff_datetime','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "folder_path = './Files/integrated_files/green/Schema_v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/green/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/green/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 2\n",
    "\n",
    "col_check = ['lpep_pickup_datetime','lpep_dropoff_datetime','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "folder_path = './Files/integrated_files/green/Schema_v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/green/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/green/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 3\n",
    "\n",
    "col_check = ['lpep_pickup_datetime','lpep_dropoff_datetime','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "folder_path = './Files/integrated_files/green/Schema_v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/green/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/green/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 4\n",
    "\n",
    "col_check = ['lpep_pickup_datetime','lpep_dropoff_datetime','trip_distance','fare_amount','mta_tax','PULocationID','DOLocationID','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['PULocationID','DOLocationID','trip_distance','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['PULocationID','DOLocationID']\n",
    "\n",
    "folder_path = './Files/integrated_files/green/Schema_v_4'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/green/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/green/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526\n"
     ]
    }
   ],
   "source": [
    "#Statistics for bad records\n",
    "x = 0\n",
    "folder_path = 'Files/integrated_files/green/dirty'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        x += df.shape[0]\n",
    "print(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the YELLOW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files/yellow/clean'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/clean')\n",
    "    \n",
    "if os.path.exists('Files/integrated_files/yellow/dirty'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/yellow/dirty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 1\n",
    "\n",
    "col_check = ['tpep_pickup_datetime','tpep_dropoff_datetime','trip_distance','fare_amt','mta_tax','pulocationid','dolocationid','tolls_amt','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','fare_amt','mta_tax','tolls_amt','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amt','congestion_surcharge']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "folder_path = './Files/integrated_files/yellow/Schema_v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mtag,0.5)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/yellow/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/yellow/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 2\n",
    "\n",
    "col_check = ['tpep_pickup_datetime','tpep_dropoff_datetime','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "col_float = ['trip_distance','mta_tax','pulocationid','dolocationid','tolls_amount']\n",
    "\n",
    "folder_path = './Files/integrated_files/yellow/Schema_v_2'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        \n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        #df.drop([152,2378,8406,11208,11351,11661,13045,13898,14575,15039,15842], inplace=True) # dropping Rows with string values\n",
    "        df = type_conversion(df, col_float, float)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/yellow/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/yellow/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 3\n",
    "\n",
    "col_check = ['tpep_pickup_datetime','tpep_dropoff_datetime','extra','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','extra','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge','extra']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "folder_path = './Files/integrated_files/yellow/Schema_v_3'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/yellow/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/yellow/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 4\n",
    "\n",
    "col_check = ['tpep_pickup_datetime','tpep_dropoff_datetime','extra','trip_distance','fare_amount','mta_tax','pulocationid','dolocationid','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['pulocationid','dolocationid','trip_distance','extra','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge','extra']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['pulocationid','dolocationid']\n",
    "\n",
    "folder_path = './Files/integrated_files/yellow/Schema_v_4'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/yellow/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/yellow/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema 5\n",
    "\n",
    "col_check = ['tpep_pickup_datetime','tpep_dropoff_datetime','extra','trip_distance','fare_amount','mta_tax','PULocationID','DOLocationID','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_abs = ['PULocationID','DOLocationID','trip_distance','extra','fare_amount','mta_tax','tolls_amount','improvement_surcharge','congestion_surcharge']\n",
    "\n",
    "col_missing = ['tolls_amount','congestion_surcharge','extra']\n",
    "\n",
    "col_missing_mta = ['mta_tax']\n",
    "\n",
    "col_missing_improvement = ['improvement_surcharge']\n",
    "\n",
    "col_loc = ['PULocationID','DOLocationID']\n",
    "\n",
    "folder_path = './Files/integrated_files/yellow/Schema_v_5'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        df = add_col(df)\n",
    "        df = repair_missing(df,col_missing,0)\n",
    "        df = repair_missing(df,col_missing_mta,0.5)\n",
    "        df = repair_missing(df,col_missing_improvement,0.3)\n",
    "        df = repair_absolute(df,col_abs)\n",
    "        df = locationid(df, col_loc)        \n",
    "        df = null_value(df,col_check)\n",
    "        df_clean, df_dirty = record_separation(df)\n",
    "        if df_clean.shape[0] > 0 :\n",
    "            df_clean.to_csv(os.path.join('Files/integrated_files/yellow/clean', file),index=False)\n",
    "        if df_dirty.shape[0] > 0 :\n",
    "            df_dirty.to_csv(os.path.join('Files/integrated_files/yellow/dirty', file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341112\n"
     ]
    }
   ],
   "source": [
    "#Statistics for bad records\n",
    "x = 0\n",
    "folder_path = 'Files/integrated_files/yellow/dirty'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df =pd.read_csv(file_path)\n",
    "        x += df.shape[0]\n",
    "print(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
