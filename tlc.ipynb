{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: Computing Foundation of Data Science\n",
    "## Project Title: Big Data \n",
    "## Authors: Kubam Ivo Mbi and Berdai Hasnae\n",
    "## Date: 30/11/2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  281\n"
     ]
    }
   ],
   "source": [
    "# Getting the necessary files ready\n",
    "import os\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files')\n",
    "    # unzip the files\n",
    "    # This line of code assumes the zipped file is in the same location as this jupyter notebook\n",
    "    import zipfile\n",
    "    \n",
    "    with zipfile.ZipFile('tlc_0.2perc.zip') as zip_ref:\n",
    "        zip_ref.extractall('Files/')\n",
    "#Checking the total number of files\n",
    "print(\"Total number of files: \", str(len(os.listdir('Files/tlc_0.2perc'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Collecting metadata, inspecting schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('TLC') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some key functions\n",
    "\n",
    "#importing necessary libraries\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "#generator function\n",
    "def file_content(main_folder):\n",
    "    file_list = os.listdir(main_folder)\n",
    "    for file_name in file_list:\n",
    "        yield file_name\n",
    "\n",
    "def record_stat_schema(filepath):\n",
    "    \"\"\"Function receives a file path of a folder containing files as its argument. For each file,\\\n",
    "    the number of records and files size is calculated and two lists are return containing all these values.\\\n",
    "    This function also checks the schema of these files and group them by their schema\"\"\"\n",
    "    \n",
    "    file_size = [] #list to hold file size \n",
    "    file_rec = [] #list to hold file records\n",
    "    schema_list = [] # list to hold all unique schema\n",
    "    current_schema = [] #\n",
    "    schema_list = []\n",
    "    version = 0\n",
    "    \n",
    "    for file in file_content(filepath):\n",
    "        file_path = os.path.join('Files/tlc_0.2perc', file)\n",
    "        df = spark.read.csv(file_path, header=True)\n",
    "        \n",
    "        #Checking file size and number of records\n",
    "        file_rec.append(df.count()) # append number of records\n",
    "       # x = os.stat(file_path).st_size\n",
    "        file_size.append(os.stat(file_path).st_size) # append each number of records\n",
    "        \n",
    "        #Checking the schema\n",
    "        \n",
    "        schema_new = list(df.columns)\n",
    "                          \n",
    "        for x in range(len(schema_new)):\n",
    "            schema_new[x] = schema_new[x].lower() # set the columns names to lowercase()\n",
    "           \n",
    "        if schema_new not in schema_list:\n",
    "            schema_list.append(schema_new)\n",
    "           \n",
    "                      \n",
    "        if schema_new == current_schema:\n",
    "            shutil.copy2(file_path,'Files/schema_v_'+ str(version))\n",
    "        elif schema_new != current_schema:\n",
    "            version += 1\n",
    "            if os.path.exists('Files/schema_v_'+ str(version)):\n",
    "                pass\n",
    "            else:\n",
    "                os.mkdir('Files/schema_v_'+ str(version))\n",
    "                shutil.copy2(file_path,'Files/schema_v_'+ str(version))\n",
    "            current_schema = schema_new\n",
    "            \n",
    "    return  np.array(file_rec) , np.array(file_size), schema_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_stat(file_rec, file_size):\n",
    "    ''' Function receivs two lists containing all file sizes and number of records respectively./\n",
    "    It then calculates all the necessary statistics.'''\n",
    "    \n",
    "    stat = {} # Dictionary to hold statistics for number of records and file sizes\n",
    "    records = {} # Dictionary to hold statistics for number of records\n",
    "    size = {} # Dictionary to hold statistics for file sizes\n",
    "    \n",
    "    for x in ['min','max','mean','25th','50th','75th','90th']:\n",
    "        if x == 'min':\n",
    "            records[x] = min(file_rec)\n",
    "            size[x] = min(file_size)\n",
    "        elif x == 'max':\n",
    "            records[x] = max(file_rec)\n",
    "            size[x] = max(file_size)\n",
    "        elif x == 'mean':\n",
    "            records[x] = np.around(np.mean(file_rec),2)\n",
    "            size[x] = np.around(np.mean(file_size),2)\n",
    "        elif x == '25th':\n",
    "            records[x] = np.around(np.percentile(file_rec,25),2)\n",
    "            size[x] = np.around(np.percentile(file_size,25),2)\n",
    "        elif x == '50th':\n",
    "            records[x] = np.around(np.percentile(file_rec,50),2)\n",
    "            size[x] = np.around(np.percentile(file_size,50),2)\n",
    "        elif x == '75th':\n",
    "            records[x] = np.around(np.percentile(file_rec,75),2)\n",
    "            size[x] = np.around(np.percentile(file_size,75),2)\n",
    "        elif x == '90th':\n",
    "            records[x] = np.around(np.percentile(file_rec,90),2)\n",
    "            size[x] = np.around(np.percentile(file_size,90),2)\n",
    "        stat['record stats'] = records\n",
    "        stat['size stats'] = size\n",
    "    return print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = record_stat_schema('./Files/tlc_0.2perc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'record stats': {'min': 15, 'max': 47703, 'mean': 17922.07, '25th': 3037.0, '50th': 19532.0, '75th': 28560.0, '90th': 31372.0}, 'size stats': {'min': 2512, 'max': 5959352, 'mean': 2152301.64, '25th': 257388.0, '50th': 1479679.0, '75th': 4181249.0, '90th': 5188938.0}}\n"
     ]
    }
   ],
   "source": [
    "cal_stat(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema analysis\n",
    "for x in range(len(c)):\n",
    "    for y in range(len(c[x])):\n",
    "        c[x][y] = c[x][y].strip() #removing leading and trailing spaces\n",
    "\n",
    "# Columns in schema 1 to 14 not in schema 15\n",
    "with open('./Files/schema_diff_others_vs_15.txt', 'w') as f:\n",
    "    for x in range(14):\n",
    "        ls_diff = []\n",
    "        for elem in c[x]:\n",
    "            if elem not in c[14]:\n",
    "                ls_diff.append(elem)\n",
    "         \n",
    "        \n",
    "        f.write('./Files/schema: '+str(x +1)+'\\n')\n",
    "        f.write(''.join(ls_diff)+'\\n')\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns in schema 15 not in 1-14\n",
    "\n",
    "with open('./Files/schema_diff_15_vs_others.txt', 'w') as f:\n",
    "    for x in range(14):\n",
    "        ls_diff2 = []\n",
    "        ls_diff2.append(set(c[14])-set(c[x]))\n",
    "        #print(ls_diff2)\n",
    "        ls_diff2 = ''.join(str(e) for e in ls_diff2)\n",
    "        f.write('schema: '+str(x +1)+'\\n')\n",
    "        f.write(ls_diff2+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common columns in schema 15 and in 1-14\n",
    "with open('./Files/schema_common.txt', 'w') as f:\n",
    "    for x in range(14):\n",
    "        ls_common = []\n",
    "        for elem in c[x]:\n",
    "            if elem in c[14]:\n",
    "                ls_common.append(elem)\n",
    "                \n",
    "        f.write('schema: '+str(x +1)+'\\n')\n",
    "        f.write(''.join(ls_common)+'\\n')\n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging files of Schema 4 and 6\n",
    "for file in os.listdir('./Files/schema_v_6'):\n",
    "    shutil.move(os.path.join('Files/schema_v_6', file),'./Files/schema_v_4')\n",
    "    \n",
    "#delete the schema v 6 folder  \n",
    "os.rmdir('./Files/schema_v_6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "# Function to fix column names\n",
    "\n",
    "def column_name_fix(df):\n",
    "    '''Function removes whitespaces and converts column names to lowercase'''\n",
    "    return [df.select('*', df[col].alias(col.strip().lower())).drop(col) for col in df.columns]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop column\n",
    "def column_drop(df,drop_list:list):\n",
    "    '''Function receives a spark dataframe and list of columns to be dropped.\n",
    "    It returns a dataframe less the columns specified to be dropped'''\n",
    "    return df.drop(*drop_list)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataFrame[dropoff_datetime: string, passenger_count: string, trip_distance: string, pickup_longitude: string, pickup_latitude: string, rate_code: string, store_and_fwd_flag: string, dropoff_longitude: string, dropoff_latitude: string, payment_type: string, fare_amount: string, surcharge: string, mta_tax: string, tip_amount: string, tolls_amount: string, total_amount: string, tpep_pickup_datetime: string]]\n"
     ]
    }
   ],
   "source": [
    "# Function to rename column\n",
    "def column_rename(df,col_list1,col_list2):\n",
    "    '''Function renames the column name of a dataframe with a provided name'''\n",
    "    return [df.select('*', df[col_list1[x]].alias(col_list2[x])).drop(col_list1[x]) for x in range(len(col_list1))]\n",
    "     \n",
    "            \n",
    "df2 = column_name(spark.read.csv('yellow_tripdata_2014-01.csv', header=True))   \n",
    "print(column_rename(df2,['pickup_datetime'],['tpep_pickup_datetime']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to a dataframe\n",
    "def column_add(df,col_list):\n",
    "    df = df.toPandas()\n",
    "    for x in col_list:\n",
    "        df[x] = \"\" \"\"\n",
    "    return spark.createDataFrame(df)\n",
    "df = column_add(df2,['x','y'])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert latitude and longitude to location ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{WARNING!!!.}}$ $\\color{red}{\\text{\" This action will clear all folders and their contents created during project.\"}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing working environment\n",
    "import shutil\n",
    "shutil.rmtree('./Files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
