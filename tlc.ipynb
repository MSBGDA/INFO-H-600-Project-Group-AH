{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: Computing Foundation of Data Science\n",
    "## Project Title: Big Data \n",
    "## Authors: Kubam Ivo Mbi and Berdai Hasnae\n",
    "## Date: 30/11/2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  281\n"
     ]
    }
   ],
   "source": [
    "# Getting the necessary files ready\n",
    "import os\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files')\n",
    "    # unzip the files\n",
    "    # This line of code assumes the zipped file is in the same location as this jupyter notebook\n",
    "    import zipfile\n",
    "    \n",
    "    with zipfile.ZipFile('tlc_0.2perc.zip') as zip_ref:\n",
    "        zip_ref.extractall('Files/')\n",
    "#Checking the total number of files\n",
    "print(\"Total number of files: \", str(len(os.listdir('Files/tlc_0.2perc'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Collecting metadata, inspecting schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('TLC') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some key functions\n",
    "\n",
    "#importing necessary libraries\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "#generator function\n",
    "def file_content(main_folder):\n",
    "    file_list = os.listdir(main_folder)\n",
    "    for file_name in file_list:\n",
    "        yield file_name\n",
    "\n",
    "def record_stat_schema(filepath):\n",
    "    \"\"\"Function receives a file path of a folder containing files as its argument. For each file,\\\n",
    "    the number of records and files size is calculated and two lists are return containing all these values.\\\n",
    "    This function also checks the schema of these files and group them by their schema\"\"\"\n",
    "    \n",
    "    file_size = [] #list to hold file size \n",
    "    file_rec = [] #list to hold file records\n",
    "    schema_list = [] # list to hold all unique schema\n",
    "    current_schema = [] #\n",
    "    schema_list = []\n",
    "    version = 0\n",
    "    \n",
    "    for file in file_content(filepath):\n",
    "        file_path = os.path.join('Files/tlc_0.2perc', file)\n",
    "        df = spark.read.csv(file_path, header=True)\n",
    "        \n",
    "        #Checking file size and number of records\n",
    "        file_rec.append(df.count()) # append number of records\n",
    "       # x = os.stat(file_path).st_size\n",
    "        file_size.append(os.stat(file_path).st_size) # append each number of records\n",
    "        \n",
    "        #Checking the schema\n",
    "        \n",
    "        schema_new = list(df.columns)\n",
    "                          \n",
    "        for x in range(len(schema_new)):\n",
    "            schema_new[x] = schema_new[x].lower() # set the columns names to lowercase()\n",
    "           \n",
    "        if schema_new not in schema_list:\n",
    "            schema_list.append(schema_new)\n",
    "           \n",
    "                      \n",
    "        if schema_new == current_schema:\n",
    "            shutil.copy2(file_path,'Files/schema_v_'+ str(version))\n",
    "        elif schema_new != current_schema:\n",
    "            version += 1\n",
    "            if os.path.exists('Files/schema_v_'+ str(version)):\n",
    "                pass\n",
    "            else:\n",
    "                os.mkdir('Files/schema_v_'+ str(version))\n",
    "                shutil.copy2(file_path,'Files/schema_v_'+ str(version))\n",
    "            current_schema = schema_new\n",
    "            \n",
    "    return  np.array(file_rec) , np.array(file_size), schema_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_stat(file_rec, file_size):\n",
    "    ''' Function receivs two lists containing all file sizes and number of records respectively./\n",
    "    It then calculates all the necessary statistics.'''\n",
    "    \n",
    "    stat = {} # Dictionary to hold statistics for number of records and file sizes\n",
    "    records = {} # Dictionary to hold statistics for number of records\n",
    "    size = {} # Dictionary to hold statistics for file sizes\n",
    "    \n",
    "    for x in ['min','max','mean','25th','50th','75th','90th']:\n",
    "        if x == 'min':\n",
    "            records[x] = min(file_rec)\n",
    "            size[x] = min(file_size)\n",
    "        elif x == 'max':\n",
    "            records[x] = max(file_rec)\n",
    "            size[x] = max(file_size)\n",
    "        elif x == 'mean':\n",
    "            records[x] = np.around(np.mean(file_rec),2)\n",
    "            size[x] = np.around(np.mean(file_size),2)\n",
    "        elif x == '25th':\n",
    "            records[x] = np.around(np.percentile(file_rec,25),2)\n",
    "            size[x] = np.around(np.percentile(file_size,25),2)\n",
    "        elif x == '50th':\n",
    "            records[x] = np.around(np.percentile(file_rec,50),2)\n",
    "            size[x] = np.around(np.percentile(file_size,50),2)\n",
    "        elif x == '75th':\n",
    "            records[x] = np.around(np.percentile(file_rec,75),2)\n",
    "            size[x] = np.around(np.percentile(file_size,75),2)\n",
    "        elif x == '90th':\n",
    "            records[x] = np.around(np.percentile(file_rec,90),2)\n",
    "            size[x] = np.around(np.percentile(file_size,90),2)\n",
    "        stat['record stats'] = records\n",
    "        stat['size stats'] = size\n",
    "    return print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving values\n",
    "file_records, file_sizes, schema_list = record_stat_schema('./Files/tlc_0.2perc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'record stats': {'min': 15, 'max': 47703, 'mean': 17922.07, '25th': 3037.0, '50th': 19532.0, '75th': 28560.0, '90th': 31372.0}, 'size stats': {'min': 2512, 'max': 5959352, 'mean': 2152301.64, '25th': 257388.0, '50th': 1479679.0, '75th': 4181249.0, '90th': 5188938.0}}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#View statistis\n",
    "print(cal_stat(file_records,file_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema analysis\n",
    "for x in range(len(schema_list)):\n",
    "    for y in range(len(schema_list[x])):\n",
    "        schema_list[x][y] = schema_list[x][y].strip() #removing leading and trailing spaces in column names\n",
    "\n",
    "# Columns in schema 1 to 14 not in schema 15\n",
    "with open('./Files/schema_diff_others_vs_15.txt', 'w') as f:\n",
    "    for x in range(14):\n",
    "        ls_diff = []\n",
    "        for elem in schema_list[x]:\n",
    "            if elem not in schema_list[14]:\n",
    "                ls_diff.append(elem)\n",
    "         \n",
    "        \n",
    "        f.write('./Files/schema: '+str(x +1)+'\\n')\n",
    "        f.write(''.join(ls_diff)+'\\n')\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns in schema 15 not in 1-14\n",
    "\n",
    "with open('./Files/schema_diff_15_vs_others.txt', 'w') as f:\n",
    "    for x in range(14):\n",
    "        ls_diff2 = []\n",
    "        ls_diff2.append(set(schema_list[14])-set(schema_list[x]))\n",
    "        #print(ls_diff2)\n",
    "        ls_diff2 = ''.join(str(e) for e in ls_diff2)\n",
    "        f.write('schema: '+str(x +1)+'\\n')\n",
    "        f.write(ls_diff2+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common columns in schema 15 and in 1-14\n",
    "with open('./Files/schema_common.txt', 'w') as f:\n",
    "    for x in range(14):\n",
    "        ls_common = []\n",
    "        for elem in schema_list[x]:\n",
    "            if elem in schema_list[14]:\n",
    "                ls_common.append(elem)\n",
    "                \n",
    "        f.write('schema: '+str(x +1)+'\\n')\n",
    "        f.write(''.join(ls_common)+'\\n')\n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files in schema 4 and 6 were found to be of the same schema. So a merging of the two folders was done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging files of Schema 4 and 6\n",
    "for file in os.listdir('./Files/schema_v_6'):\n",
    "    shutil.move(os.path.join('Files/schema_v_6', file),'./Files/schema_v_4')\n",
    "    \n",
    "#delete the schema v 6 folder  \n",
    "os.rmdir('./Files/schema_v_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder created to hold zipped is deleted.\n",
    "# Clearing working environment\n",
    "import shutil\n",
    "shutil.rmtree('./Files/tlc_0.2perc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "# Function to fix column names\n",
    "\n",
    "def column_name_fix(df):\n",
    "    '''Function converts column names to lowercase'''\n",
    "    colnames = df.columns\n",
    "    for x in range(len(colnames)):\n",
    "        colnames[x] =  colnames[x].lower()\n",
    "    df = df.toDF(*colnames)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop column\n",
    "def column_drop(df,drop_list):\n",
    "    '''Function receives a spark dataframe and list of columns to be dropped.\n",
    "    It returns a dataframe less the columns specified to be dropped'''\n",
    "    y = []\n",
    "    for x in drop_list:\n",
    "        y.append(df.columns[x])\n",
    "    return df.drop(*y)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename column\n",
    "def column_rename(df,col_list1,col_list2):\n",
    "    '''Function renames the column name of a dataframe with a provided list of column names.\n",
    "        The two lists must be of the order i.e. value one in list one replaces value one in list two and so on'''\n",
    "    if len(col_list1) == len(col_list2):\n",
    "        for x in range(len(col_list1)):\n",
    "            df = df.select('*', df[col_list1[x]].alias(col_list2[x]))\n",
    "        df = column_drop(df,col_list1)\n",
    "        return df\n",
    "    else:\n",
    "        print('length of two list must be thesame.')\n",
    "        pass\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to a dataframe\n",
    "from pyspark.sql.functions import lit\n",
    "def column_add(df,col_list):\n",
    "    \n",
    "    for x in col_list:\n",
    "        df = df.withColumn(x, lit(\"\"))\n",
    "    return df\n",
    "\n",
    "df = spark.read.csv('fhvhv_tripdata_2019-02.csv', header=True)\n",
    "col_list = ['fare_amount', 'improvement_surcharge', 'congestion_surcharge', 'passenger_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert latitude and longitude to location ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the files Based on the schema differences identified above. The schema of folder 16 will be used as the standard or\n",
    "baseline schema because it is the latest. So changess are made to files based on their current schema differences with the baseline schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a folder to hold integrated files\n",
    "# Check wether folder exist if not create\n",
    "if os.path.exists('Files/integrated_files'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Files/integrated_files/FHV/Schema_v_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0585ea550a9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Files/integrated_files/FHV/Schema_v_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Columns to be renamed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Files/integrated_files/FHV/Schema_v_1'"
     ]
    }
   ],
   "source": [
    "# Schema One\n",
    "import os\n",
    "if os.path.exists('Files/integrated_files/FHV/Schema_v_1'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('Files/integrated_files/FHV/Schema_v_1')\n",
    "\n",
    "# Columns to be renamed\n",
    "col_list1 = [2, 3]\n",
    "col_list2 = ['tpep_pickup_datetime','tpep_dropoff_datetime']\n",
    "# columns to be added\n",
    "col_add = ['fare_amount', 'improvement_surcharge', 'congestion_surcharge', 'passenger_count', 'tolls_amount', 'extra', 'payment_type', 'store_and_fwd_flag', 'total_amount', 'ratecodeid', 'mta_tax', 'trip_distance', 'tpep_dropoff_datetime', 'tpep_pickup_datetime', 'tip_amount', 'vendorid']\n",
    "folder_path = 'Files/schema_v_1'\n",
    "for file in file_content(folder_path):\n",
    "        file_path = os.path.join('Files/schema_v_1', file)\n",
    "        df = column_name_fix(spark.read.csv(file_path, header=True))# The read process pass via column_name_fix function\n",
    "        df = column_rename(df,col_list1,col_list2)\n",
    "        df = column_add(df, col_add)\n",
    "        df.toPandas().to_csv(os.path.join('Files/integrated_files/FHV/Schema_v_1', file))\n",
    "        # write code to export to csv        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{WARNING!!!.}}$ $\\color{red}{\\text{\" This action will clear all folders and their contents created during project.\"}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing working environment\n",
    "import shutil\n",
    "shutil.rmtree('./Files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
